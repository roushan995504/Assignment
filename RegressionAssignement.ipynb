{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6hsq92wfAO7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Simple Linear Regression"
      ],
      "metadata": {
        "id": "s2Tq2hcJgNQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression is a statistical method used to model the relationship between two variables: one independent variable (predictor or explanatory variable) and one dependent variable (response variable). It is called \"simple\" because it involves only one independent variable. The relationship between these variables is modeled by fitting a straight line, represented by the equation:\n",
        "\n",
        "ð‘¦=ð‘šð‘¥+ð‘\n",
        "\n",
        "Where:\n",
        "ð‘¦ is the dependent variable (response).\n",
        "x is the independent variable (predictor).\n",
        "ð‘š is the slope of the line (the change in\n",
        "ð‘¦ for a unit change in\n",
        "m is the slope of the line (the change in y for a unit change in x).\n",
        "\n",
        "Objective\n",
        "The goal of simple linear regression is to find the line that best fits the data points, minimizing the differences between the observed values (\n",
        "ð‘¦\n",
        "y) and the predicted values (y^ ) from the model. This is typically achieved by minimizing the sum of squared residuals (errors), where residuals are the differences between the observed and predicted values.\n",
        "\n",
        "Assumptions\n",
        "Linearity: The relationship between the independent and dependent variables is linear.\n",
        "Independence: Observations are independent of each other.\n",
        "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable.\n",
        "Normality: The residuals are normally distributed."
      ],
      "metadata": {
        "id": "V0q8PLwWfEXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression"
      ],
      "metadata": {
        "id": "x2p27mk7gTu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assumptions\n",
        "Linearity: The relationship between the independent and dependent variables is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable.\n",
        "\n",
        "Normality: The residuals are normally distributed."
      ],
      "metadata": {
        "id": "5iDOZtn5gbFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c"
      ],
      "metadata": {
        "id": "PknM6MvTgpgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficient ð‘š in the equation ð‘Œ =ð‘šð‘‹+ð‘\n",
        "Y=mX+c represents the slope of the line, which quantifies the rate of change of the dependent variable\n",
        "ð‘Œ with respect to the independent variable X.\n",
        "\n",
        "Rate of Change:\n",
        "\n",
        "m indicates how much Y changes for a one-unit increase in X.\n",
        "For example, if\n",
        "ð‘š = 2 then Y increases by 2 units for every 1-unit increase in X.\n",
        "\n",
        "\n",
        "Direction of Relationship:\n",
        "If m>0: The relationship is positive (as X increases, Y increases).\n",
        "\n",
        "If m<0:  The relationship is negative (as x increases, Y decreases).\n",
        "\n",
        "if m==0: y does not change with x implying no relation."
      ],
      "metadata": {
        "id": "Ua3US3NzgtyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5b2dIHbIhmdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c\n",
        "\n",
        "the intercept c in equation y=mx+c represent y intercept when x = 0.\n",
        "c indicates the starting point of Y on the y-axis when X=0."
      ],
      "metadata": {
        "id": "PjsAc20fiajO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression\n",
        "\n",
        "In Simple Linear Regression, the slope m (also called the regression coefficient) is calculated using the formula:\n",
        "m = âˆ‘(Xi-mean(x))(Yi-mean(y))/âˆ‘(Xi-mean(x))^2\n",
        "\n",
        "where:\n",
        "Xi :Individual data points for the independent variable.\n",
        "\n",
        "Yi : Individual data points for the dependent variable.\n",
        "\n",
        "mean(x) :  Mean of the X-values.\n",
        "\n",
        "mean(y) : Mean of the y value\n",
        "\n"
      ],
      "metadata": {
        "id": "1q__IPSfhPHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression\n",
        "\n",
        "The least squares method in Simple Linear Regression is a mathematical approach used to find the best-fitting line through a set of data points. The goal is to minimize the sum of the squared differences between the observed values (yi) and the predicted values (y^i) from the regression line. These differences are called residuals or errors.\n",
        "\n",
        "Purpose of the Least Squares Method:\n",
        "\n",
        "Minimizing Errors:\n",
        "\n",
        "he method ensures that the total squared residuals âˆ‘(ð‘¦ð‘–âˆ’ð‘¦^ð‘–)^2 is as small as possible. This creates a line that best represents the data.\n",
        "\n",
        "Optimal Fit:\n",
        "\n",
        "he regression line calculated using this method is the one that minimizes the vertical distances between the actual data points and the line. This ensures that the line provides the most accurate predictions for y given x.\n",
        "\n",
        "Balancing Positive and Negative Errors:\n",
        "\n",
        "By squaring the residuals, the method avoids the problem of positive and negative residuals canceling each other out, which would distort the accuracy of the fit.\n",
        "\n",
        "4. Predictive Accuracy\n",
        "The least squares line is designed to maximize the accuracy of predictions for y based on new values of x."
      ],
      "metadata": {
        "id": "saoMgRufk_R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression\n",
        "\n",
        "The coefficient of determination(R^2) is a statistical measure that explains how well the regression line represents the data. It indicates the proportion of the variance in the dependent variable y that is explained by the independent variable x in the model.\n"
      ],
      "metadata": {
        "id": "UMlnqxS1mq6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "8. What is Multiple Linear Regression\n",
        "\n",
        "Multiple Linear Regression is an extension of Simple Linear Regression where the dependent variable y is predicted using multiple independent variables (x1,x2,x3,x4,...) Instead of using just one feature to make predictions, multiple features are used to model the relationship between the predictors and the response variable.\n",
        "\n",
        "Purpose of Multiple Linear Regression:\n",
        "\n",
        "Understand Relationships:\n",
        "\n",
        "Analyze how each independent variable affects the dependent variable.\n",
        "Determine which predictors are most significant.\n",
        "\n",
        "Use the regression equation to predict ð‘Œ for new value of x1,x2,x2,..\n",
        "\n",
        "Measure the impact of each independent variable while keeping others constant.\n"
      ],
      "metadata": {
        "id": "YWqTKugxn9Pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression\n",
        "\n",
        "The main difference between Simple Linear Regression (SLR) and Multiple Linear Regression (MLR) lies in the number of independent variables (predictors) used to predict the dependent variable:\n",
        "\n",
        "1. Number of Independent Variables:\n",
        "\n",
        "Simple Linear Regression: Uses one independent variable to predict the dependent variable.\n",
        "\n",
        "Multiple Linear Regression: Uses two or more independent variables to predict the dependent variable.\n",
        "\n",
        "2. Complexity:\n",
        "SLR:\n",
        "\n",
        "Simpler to calculate and interpret as it involves only one predictor.\n",
        "\n",
        "Visualized as a straight line in a 2D plot.\n",
        "\n",
        "MLR:\n",
        "\n",
        "More complex as it involves multiple predictors.\n",
        "\n",
        "Visualized in higher dimensions (e.g., a plane in 3D, hyperplanes in higher dimensions).\n",
        "\n",
        "Predictive Power:\n",
        "SLR: Limited to explaining the effect of a single variable, so it may overlook other important factors influencing the dependent variable.\n",
        "\n",
        "\n",
        "MLR: Accounts for multiple factors, providing a more comprehensive and accurate prediction."
      ],
      "metadata": {
        "id": "qBmTl5epoylQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 What are the key assumptions of Multiple Linear Regression\n",
        "\n",
        "The key assumptions of Multiple Linear Regression (MLR) ensure that the model produces reliable, unbiased results and valid predictions. These assumptions include:\n",
        "\n",
        "1. Linearity :\n",
        "\n",
        "The relationship between the dependent variable y and each independent variable (x1,x2,x3,x4,...) is linear.\n",
        "\n",
        "This means that changes in the independent variables result in proportional changes in the dependent variable.\n",
        "\n",
        "2. Homoscedasticity\n",
        "The variance of the residuals (Ïµ) should remain constant across all values of the independent variables.\n",
        "\n",
        "If the residuals have a pattern (e.g., they fan out or cluster), it indicates heteroscedasticity, which violates this assumption."
      ],
      "metadata": {
        "id": "EJSk4rxaplg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "\n",
        "Heteroscedasticity and Its Effect on Multiple Linear Regression\n",
        "Heteroscedasticity refers to the condition where the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variables. In the presence of heteroscedasticity, the assumption of constant variance (homoscedasticity) is violated, which can lead to inefficient estimates and invalid statistical tests (e.g., biased significance tests), affecting the reliability of the model's results."
      ],
      "metadata": {
        "id": "xu-xU1SVqeQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "\n",
        "To improve a model with high multicollinearity, you can:\n",
        "\n",
        "Remove or combine correlated variables to reduce redundancy.\n",
        "Use regularization techniques like Ridge or Lasso regression, which can penalize high correlation among features.\n",
        "Principal Component Analysis (PCA) can be used to transform the data into uncorrelated components.\n",
        "Increase sample size to reduce the variance of coefficient estimates.\n"
      ],
      "metadata": {
        "id": "HJ4GY59frDa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models\n",
        "\n",
        "Techniques for Transforming Categorical Variables for Regression Models\n",
        "Common techniques include:\n",
        "\n",
        "One-Hot Encoding: Converts categorical variables into a binary matrix where each category becomes a separate column.\n",
        "\n",
        "Label Encoding: Assigns an integer to each category, suitable for ordinal data.\n",
        "\n",
        "Binary Encoding: Uses binary digits to represent categories, effective for large numbers of categories.\n",
        "\n",
        "Target Encoding: Replaces categories with the mean of the target variable, useful when the number of categories is large.\n"
      ],
      "metadata": {
        "id": "ShkM_7o4rSNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Role of Interaction Terms in Multiple Linear Regression\n",
        "Interaction terms in Multiple Linear Regression capture the combined effect of two or more independent variables on the dependent variable, beyond their individual effects. These terms help model more complex relationships between predictors and the outcome."
      ],
      "metadata": {
        "id": "ugHPmscdrZZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "\n",
        "In Simple Linear Regression, the intercept represents the expected value of the dependent variable when the independent variable is zero. In Multiple Linear Regression, the intercept is the predicted value of the dependent variable when all independent variables are zero. However, its interpretation in multiple regression may not always make sense if having all predictors at zero is not realistic."
      ],
      "metadata": {
        "id": "NEbT2ICnrl5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "\n",
        "The slope represents the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant. In predictions, the slope helps quantify the impact of each predictor on the outcome, allowing for understanding of the relationship between variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "FE5dxUucrtoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.How does the intercept in a regression model provide context for the relationship between variables\n",
        "\n",
        "The intercept in a regression model provides the baseline or starting value for the dependent variable when all independent variables are zero. It gives context to the relationship between variables, especially in multiple regression where it reflects the effect of all other predictors being held constant.\n",
        "\n"
      ],
      "metadata": {
        "id": "92IzFoJcr4Tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using RÂ² as a sole measure of model performance\n",
        "\n",
        "While RÂ² measures the proportion of variance explained by the model, it doesn't indicate whether the model is a good fit. RÂ² can be misleading, especially in models with many predictors, as it tends to increase with the number of predictors, even if the model is overfitting. This is why adjusted RÂ² and other metrics are preferred."
      ],
      "metadata": {
        "id": "E0Z--tA8sApy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient\n",
        "\n",
        "A large standard error indicates that the coefficient estimate is imprecise and may not be statistically significant. It suggests that there is high variability in the coefficient's estimated value across different samples, and thus, caution should be taken when making inferences from such a coefficient."
      ],
      "metadata": {
        "id": "PM_KAHwWsLBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "\n",
        "Heteroscedasticity can be identified in residual plots by looking for patterns where the spread of residuals increases or decreases with the fitted values or an independent variable. To address heteroscedasticity, you can apply transformations to the dependent variable (e.g., log transformation) or use robust standard errors."
      ],
      "metadata": {
        "id": "Enz4M5GBsPj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²\n",
        "\n",
        "This situation often occurs when the model has too many predictors, leading to overfitting. While RÂ² increases with more predictors, the adjusted RÂ² accounts for the number of predictors and penalizes the addition of irrelevant ones. A large discrepancy between the two suggests the model might not generalize well.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sv7SF20-sbcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Why is it important to scale variables in Multiple Linear Regression\n",
        "\n",
        "Scaling ensures that each variable contributes equally to the model. Features with large ranges can disproportionately influence the model, especially when using regularization techniques like Ridge or Lasso regression. Standardizing variables (e.g., using z-scores) helps in improving the model's performance."
      ],
      "metadata": {
        "id": "p_vWaKsOse0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.What is polynomial regression\n",
        "\n",
        "Polynomial regression is a type of regression where the relationship between the independent and dependent variable is modeled as an nth-degree polynomial. It is used when the data exhibits a non-linear relationship."
      ],
      "metadata": {
        "id": "EQbwUWXlsqhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression\n",
        "\n",
        "Linear regression assumes a straight-line relationship between variables, whereas polynomial regression allows for curves in the relationship by introducing higher-degree terms of the predictors (e.g., xÂ², xÂ³).\n",
        "\n"
      ],
      "metadata": {
        "id": "mtLjnoROsxAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used\n",
        "\n",
        "Polynomial regression is used when the data shows a curvilinear relationship, and a linear model would not adequately capture the complexity of the data. It is useful for modeling trends like quadratic, cubic, and higher-order relationships.\n"
      ],
      "metadata": {
        "id": "dZ46DhhUs5w_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.What is the general equation for polynomial regression\n",
        "\n",
        "y = Î²0+Î²1x+Î²2x^2+...\n"
      ],
      "metadata": {
        "id": "mRqzLUC6s9E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables\n",
        "Yes, polynomial regression can be applied to multiple variables by considering interaction terms and polynomial features of each independent variable. The model would include terms like"
      ],
      "metadata": {
        "id": "qCN6qHYotmC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression\n",
        "\n",
        "Polynomial regression can lead to overfitting if the degree of the polynomial is too high. It can also become computationally expensive and harder to interpret, especially with many predictors. Additionally, polynomial terms can introduce multicollinearity"
      ],
      "metadata": {
        "id": "tHttFAWctuuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "\n",
        "Methods include:\n",
        "\n",
        "Cross-validation: Evaluates the model's performance on unseen data to avoid overfitting.\n",
        "\n",
        "Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC): Measures the goodness of fit while penalizing complexity.\n",
        "Adjusted RÂ²: A version of RÂ² that accounts for the number of predictors, useful when comparing models with different degrees."
      ],
      "metadata": {
        "id": "k3lgacFyt2Qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression\n",
        "Visualization helps in understanding the fit of the polynomial model to the data. It allows you to visually assess how well the model captures the non-linear relationships and check for potential overfitting or underfitting\n"
      ],
      "metadata": {
        "id": "GdzmmS-buAuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.How is polynomial regression implemented in Python\n",
        "\n",
        "To implement polynomial regression in Python, you can use scikit-learn, a popular machine learning library. Here's a step-by-step guide on how to implement polynomial regression:"
      ],
      "metadata": {
        "id": "qSu1zuDDuEhn"
      }
    }
  ]
}